# SPDX-FileCopyrightText: Copyright (c) 2025 ROBOTIS CO., LTD.
# SPDX-License-Identifier: Apache-2.0
#
# Multi-camera inference data config for FFW SG2 Rev1 robot.
#
# This config supports inference with 3 cameras:
# - video.ego_view (head camera, 672x376)
# - video.cam_wrist_left (left wrist, 424x240)
# - video.cam_wrist_right (right wrist, 424x240)
#
# CRITICAL: Uses VideoResizeIndependent to resize each camera to 224x224
# BEFORE concatenation, allowing different native resolutions at runtime.

import sys
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Any, Dict
import cv2
import numpy as np

# Add Isaac-GR00T to path if not already there
ISAAC_GROOT_PATH = Path("/workspace/Isaac-GR00T")
if ISAAC_GROOT_PATH.exists() and str(ISAAC_GROOT_PATH) not in sys.path:
    sys.path.insert(0, str(ISAAC_GROOT_PATH))

from gr00t.data.dataset import ModalityConfig
from gr00t.data.transform.base import ComposedModalityTransform, ModalityTransform
from gr00t.data.transform.concat import ConcatTransform
from gr00t.data.transform.state_action import (
    StateActionSinCosTransform,
    StateActionToTensor,
    StateActionTransform,
)
from gr00t.model.transforms import GR00TTransform
import torch
from gr00t.experiment.data_config import BaseDataConfig


class VideoToTensorSimple(ModalityTransform):
    """Simple video to tensor conversion WITHOUT resolution checks.
    
    The standard GR00T VideoToTensor checks if input resolution matches
    the original training resolution, which fails when we pre-resize.
    This version skips that check.
    """
    
    apply_to: List[str]
    
    def apply(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Convert numpy video arrays to torch tensors."""
        for key in self.apply_to:
            if key not in data:
                continue
            
            video = data[key]
            
            # Ensure numpy array
            if not isinstance(video, np.ndarray):
                continue
            
            # Convert to torch tensor
            # Input shape: (T, H, W, C) or (B, T, H, W, C)
            # Keep the same shape, just convert dtype
            tensor = torch.from_numpy(video.copy())
            data[key] = tensor
        
        return data


class VideoToNumpySimple(ModalityTransform):
    """Simple tensor to numpy conversion WITHOUT resolution checks."""
    
    apply_to: List[str]
    
    def apply(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Convert torch tensors to numpy arrays."""
        for key in self.apply_to:
            if key not in data:
                continue
            
            video = data[key]
            
            if isinstance(video, torch.Tensor):
                data[key] = video.numpy()
        
        return data


class VideoResizeIndependent(ModalityTransform):
    """Resize each video key independently to the same resolution.
    
    This transform resizes each camera view BEFORE other video transforms
    concatenate them together. This allows using multiple cameras with
    different original resolutions.
    
    Unlike the standard VideoResize which concatenates first then resizes,
    this transform resizes each view independently.
    
    Args:
        apply_to: List of video keys to apply transform to
        height: Target height
        width: Target width
        interpolation: Interpolation method ("nearest", "linear", "cubic", "area", "lanczos")
    """
    
    apply_to: List[str]
    height: int
    width: int
    interpolation: str = "linear"
    
    _INTERPOLATION_MAP = {
        "nearest": cv2.INTER_NEAREST,
        "linear": cv2.INTER_LINEAR,
        "cubic": cv2.INTER_CUBIC,
        "area": cv2.INTER_AREA,
        "lanczos": cv2.INTER_LANCZOS4,
    }
    
    def apply(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Resize each video key independently.
        
        Input: numpy array with dtype uint8
        Output: numpy array with dtype uint8, shape (T, H, W, C)
        """
        interp = self._INTERPOLATION_MAP.get(self.interpolation, cv2.INTER_LINEAR)
        
        for key in self.apply_to:
            if key not in data:
                continue
            
            video = data[key]
            original_dtype = video.dtype
            
            # Handle different input shapes
            # video shape: (T, H, W, C) or (B, T, H, W, C)
            if video.ndim == 5:
                # (B, T, H, W, C) -> process each batch
                resized = []
                for b in range(video.shape[0]):
                    batch_frames = []
                    for t in range(video.shape[1]):
                        frame = video[b, t]
                        resized_frame = cv2.resize(frame, (self.width, self.height), interpolation=interp)
                        batch_frames.append(resized_frame)
                    resized.append(np.stack(batch_frames, axis=0))
                result = np.stack(resized, axis=0)
            elif video.ndim == 4:
                # (T, H, W, C) -> resize each frame
                resized_frames = []
                for t in range(video.shape[0]):
                    frame = video[t]
                    resized_frame = cv2.resize(frame, (self.width, self.height), interpolation=interp)
                    resized_frames.append(resized_frame)
                result = np.stack(resized_frames, axis=0)
            elif video.ndim == 3:
                # Single frame (H, W, C) -> add time dimension for consistency
                resized = cv2.resize(video, (self.width, self.height), interpolation=interp)
                result = resized[np.newaxis, ...]  # (1, H, W, C)
            else:
                raise ValueError(f"Unsupported video shape {video.shape} for key {key}")
            
            # Ensure contiguous array with correct dtype
            result = np.ascontiguousarray(result, dtype=original_dtype)
            data[key] = result
        
        return data


@dataclass
class FFWSG2MultiCamInferenceConfig(BaseDataConfig):
    """Multi-camera inference config for FFW SG2 Rev1 dual-arm robot.
    
    This config is designed for real-time inference with 3 cameras that may
    have different native resolutions:
    - video.ego_view: 672x376 (head camera)
    - video.cam_wrist_left: 424x240 (left wrist)
    - video.cam_wrist_right: 424x240 (right wrist)
    
    It uses VideoResizeIndependent to resize each camera to 224x224 BEFORE
    concatenation, handling the resolution mismatch at runtime.
    
    State Configuration (16 DOF arms + grippers):
    - state.left_arm: 7 joints
    - state.left_hand: 1 gripper
    - state.right_arm: 7 joints
    - state.right_hand: 1 gripper
    
    Action Configuration (same as state):
    - action.left_arm: 7 joints
    - action.left_hand: 1 gripper
    - action.right_arm: 7 joints
    - action.right_hand: 1 gripper
    """
    
    # Camera keys - 3 cameras for multi-cam inference
    video_keys: List[str] = field(default_factory=lambda: [
        "video.ego_view",
        "video.cam_wrist_left", 
        "video.cam_wrist_right"
    ])
    
    # State keys for the dual-arm configuration
    state_keys: List[str] = field(default_factory=lambda: [
        "state.left_arm",
        "state.left_hand",
        "state.right_arm",
        "state.right_hand",
    ])
    
    # Action keys
    action_keys: List[str] = field(default_factory=lambda: [
        "action.left_arm",
        "action.left_hand",
        "action.right_arm",
        "action.right_hand",
    ])
    
    # Language key for task instruction
    language_keys: List[str] = field(default_factory=lambda: [
        "annotation.human.action.task_description"
    ])
    
    # Observation and action horizons (same as training)
    observation_indices: List[int] = field(default_factory=lambda: [0])
    action_indices: List[int] = field(default_factory=lambda: list(range(16)))
    
    # Target resolution for all cameras
    target_height: int = 224
    target_width: int = 224

    def modality_config(self) -> dict:
        """Return modality configuration for the model."""
        video_modality = ModalityConfig(
            delta_indices=self.observation_indices,
            modality_keys=self.video_keys,
        )
        state_modality = ModalityConfig(
            delta_indices=self.observation_indices,
            modality_keys=self.state_keys,
        )
        action_modality = ModalityConfig(
            delta_indices=self.action_indices,
            modality_keys=self.action_keys,
        )
        language_modality = ModalityConfig(
            delta_indices=self.observation_indices,
            modality_keys=self.language_keys,
        )
        return {
            "video": video_modality,
            "state": state_modality,
            "action": action_modality,
            "language": language_modality,
        }

    def transform(self) -> ModalityTransform:
        """Return the modality transform for multi-cam inference.
        
        CRITICAL: Uses VideoResizeIndependent FIRST to resize each camera
        independently to 224x224, handling different native resolutions.
        
        Pipeline:
        1. VideoResizeIndependent: numpy (any size) → numpy (224x224)
        2. VideoToTensorSimple: numpy → torch tensor (NO resolution check!)
        3. VideoToNumpySimple: torch tensor → numpy
        
        NOTE: We use custom Simple transforms instead of GR00T's VideoToTensor/VideoToNumpy
        because GR00T's versions have resolution checks that fail after pre-resize.
        """
        transforms = [
            # STEP 1: Resize each camera INDEPENDENTLY to target size (numpy → numpy)
            # This handles different original resolutions:
            #   ego_view: 672x376 → 224x224
            #   cam_wrist_left: 424x240 → 224x224
            #   cam_wrist_right: 424x240 → 224x224
            VideoResizeIndependent(
                apply_to=self.video_keys,
                height=self.target_height,
                width=self.target_width,
                interpolation="linear",
            ),
            # STEP 2: Convert resized numpy to tensor (NO resolution check!)
            VideoToTensorSimple(apply_to=self.video_keys),
            # STEP 3: Convert back to numpy
            VideoToNumpySimple(apply_to=self.video_keys),
            # State transforms
            StateActionToTensor(apply_to=self.state_keys),
            StateActionSinCosTransform(apply_to=self.state_keys),
            # Action transforms
            StateActionToTensor(apply_to=self.action_keys),
            StateActionTransform(
                apply_to=self.action_keys,
                normalization_modes={key: "min_max" for key in self.action_keys},
            ),
            # Concat transforms - combines all 3 cameras
            ConcatTransform(
                video_concat_order=self.video_keys,
                state_concat_order=self.state_keys,
                action_concat_order=self.action_keys,
            ),
            # Model-specific transform
            GR00TTransform(
                state_horizon=len(self.observation_indices),
                action_horizon=len(self.action_indices),
                max_state_dim=64,
                max_action_dim=32,
            ),
        ]
        return ComposedModalityTransform(transforms=transforms)


# Single-camera config variant for flexibility
@dataclass
class FFWSG2SingleCamInferenceConfig(FFWSG2MultiCamInferenceConfig):
    """Single-camera inference config - backward compatible with single ego_view."""
    
    video_keys: List[str] = field(default_factory=lambda: ["video.ego_view"])
